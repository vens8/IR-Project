{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ycKanoS9bY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# load english language model\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
        "\n",
        "text = \"This is a sample sentence.\"\n",
        "\n",
        "# create spacy \n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text,'->',token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The children love cream biscuits\"\n",
        "\n",
        "# create spacy \n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text,'->',token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_T4wHT9UBmw",
        "outputId": "b7d9923e-50d6-4d2f-bfab-195bf1148def"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The -> DET\n",
            "children -> NOUN\n",
            "love -> VERB\n",
            "cream -> NOUN\n",
            "biscuits -> NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "displacy.render(doc, style='dep',jupyter=True)\n",
        "\n",
        "for token in doc:\n",
        "    # extract subject\n",
        "    if (token.dep_=='nsubj'):\n",
        "        print(token.text)\n",
        "    # extract object\n",
        "    elif (token.dep_=='dobj'):\n",
        "        print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "5s5-JQD1UHVU",
        "outputId": "05db61de-d414-4ca5-88fa-11667ca22cae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9999df9145cf4b76afa3617d4c962c68-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">children</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">love</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">cream</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">biscuits</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9999df9145cf4b76afa3617d4c962c68-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9999df9145cf4b76afa3617d4c962c68-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9999df9145cf4b76afa3617d4c962c68-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9999df9145cf4b76afa3617d4c962c68-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9999df9145cf4b76afa3617d4c962c68-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9999df9145cf4b76afa3617d4c962c68-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9999df9145cf4b76afa3617d4c962c68-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9999df9145cf4b76afa3617d4c962c68-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "children\n",
            "biscuits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Folder path\n",
        "folders = glob.glob('./UNGD/UNGDC 1970-2018/Converted sessions/Session*')\n",
        "\n",
        "# Dataframe\n",
        "df = pd.DataFrame(columns={'Country','Speech','Session','Year'})\n",
        "\n",
        "# Read speeches by India\n",
        "i = 0 \n",
        "for file in folders:\n",
        "    \n",
        "    speech = glob.glob(file+'/IND*.txt')\n",
        "\n",
        "    with open(speech[0],encoding='utf8') as f:\n",
        "        # Speech\n",
        "        df.loc[i,'Speech'] = f.read()\n",
        "        # Year \n",
        "        df.loc[i,'Year'] = speech[0].split('_')[-1].split('.')[0]\n",
        "        # Session\n",
        "        df.loc[i,'Session'] = speech[0].split('_')[-2]\n",
        "        # Country\n",
        "        df.loc[i,'Country'] = speech[0].split('_')[0].split(\"\\\\\")[-1]\n",
        "        # Increment counter\n",
        "        i += 1 \n",
        "        \n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "NCp2-gzAv7Ej",
        "outputId": "246a3e09-96a8-4d97-bc52-ee492b67c42c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Year, Session, Speech, Country]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d5cc19a-9cd4-4697-b51d-9c4ee0358083\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Session</th>\n",
              "      <th>Speech</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d5cc19a-9cd4-4697-b51d-9c4ee0358083')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d5cc19a-9cd4-4697-b51d-9c4ee0358083 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d5cc19a-9cd4-4697-b51d-9c4ee0358083');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    \n",
        "    # removing paragraph numbers\n",
        "    text = re.sub('[0-9]+.\\t','',str(text))\n",
        "    # removing new line characters\n",
        "    text = re.sub('\\n ','',str(text))\n",
        "    text = re.sub('\\n',' ',str(text))\n",
        "    # removing apostrophes\n",
        "    text = re.sub(\"'s\",'',str(text))\n",
        "    # removing hyphens\n",
        "    text = re.sub(\"-\",' ',str(text))\n",
        "    text = re.sub(\"— \",'',str(text))\n",
        "    # removing quotation marks\n",
        "    text = re.sub('\\\"','',str(text))\n",
        "    # removing salutations\n",
        "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
        "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
        "    # removing any reference to outside text\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
        "    \n",
        "    return text\n",
        "\n",
        "# preprocessing speeches\n",
        "df['Speech_clean'] = df['Speech'].apply(clean)\n"
      ],
      "metadata": {
        "id": "VyppNzjSUJmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences(text):\n",
        "    # split sentences and questions\n",
        "    text = re.split('[.?]', text)\n",
        "    clean_sent = []\n",
        "    for sent in text:\n",
        "        clean_sent.append(sent)\n",
        "    return clean_sent\n",
        "\n",
        "# sentences\n",
        "df['sent'] = df['Speech_clean'].apply(sentences)"
      ],
      "metadata": {
        "id": "PK7Tp42QfZor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# create a dataframe containing sentences\n",
        "df2 = pd.DataFrame(columns=['Sent','Year','Len'])\n",
        "\n",
        "row_list = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    for sent in df.loc[i,'sent']:\n",
        "    \n",
        "        wordcount = len(sent.split())\n",
        "        year = df.loc[i,'Year']\n",
        "\n",
        "        dict1 = {'Year':year,'Sent':sent,'Len':wordcount}\n",
        "        row_list.append(dict1)\n",
        "    \n",
        "df2 = pd.DataFrame(row_list)"
      ],
      "metadata": {
        "id": "QSU5-E_IfdKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "\n",
        "from spacy import displacy \n",
        "import visualise_spacy_tree\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# load english language model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])"
      ],
      "metadata": {
        "id": "E_GMFyHIfflr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to find sentences containing PMs of India\n",
        "def find_names(text):\n",
        "    \n",
        "    names = []\n",
        "    \n",
        "    # spacy doc\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # pattern\n",
        "    pattern = [{'LOWER':'prime'},\n",
        "              {'LOWER':'minister'},\n",
        "              {'POS':'ADP','OP':'?'},\n",
        "              {'POS':'PROPN'}]\n",
        "                \n",
        "    # Matcher class object \n",
        "    matcher = Matcher(nlp.vocab) \n",
        "    matcher.add(\"names\", None, pattern) \n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    # finding patterns in the text\n",
        "    for i in range(0,len(matches)):\n",
        "        \n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        names.append(str(token))\n",
        "    \n",
        "    # Only keep sentences containing Indian PMs\n",
        "    for name in names:\n",
        "        if (name.split()[2] == 'of') and (name.split()[3] != \"India\"):\n",
        "                names.remove(name)\n",
        "            \n",
        "    return names\n",
        "\n",
        "# apply function\n",
        "df2['PM_Names'] = df2['Sent'].apply(find_names)"
      ],
      "metadata": {
        "id": "gE08LBrjfh1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check if keyswords like 'programs','schemes', etc. present in sentences\n",
        "\n",
        "def prog_sent(text):\n",
        "    \n",
        "    patterns = [r'\\b(?i)'+'plan'+r'\\b',\n",
        "               r'\\b(?i)'+'programme'+r'\\b',\n",
        "               r'\\b(?i)'+'scheme'+r'\\b',\n",
        "               r'\\b(?i)'+'campaign'+r'\\b',\n",
        "               r'\\b(?i)'+'initiative'+r'\\b',\n",
        "               r'\\b(?i)'+'conference'+r'\\b',\n",
        "               r'\\b(?i)'+'agreement'+r'\\b',\n",
        "               r'\\b(?i)'+'alliance'+r'\\b']\n",
        "\n",
        "    output = []\n",
        "    flag = 0\n",
        "    for pat in patterns:\n",
        "        if re.search(pat, text) != None:\n",
        "            flag = 1\n",
        "            break\n",
        "    return flag \n",
        "\n",
        "# apply function\n",
        "df2['Check_Schemes'] = df2['Sent'].apply(prog_sent)\n"
      ],
      "metadata": {
        "id": "tSyw3wkwfkYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to extract initiatives using pattern matching\n",
        "def all_schemes(text,check):\n",
        "    \n",
        "    schemes = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # initiatives\n",
        "    prog_list = ['programme','scheme',\n",
        "                 'initiative','campaign',\n",
        "                 'agreement','conference',\n",
        "                 'alliance','plan']\n",
        "    \n",
        "    # pattern to match initiatives names \n",
        "    pattern = [{'POS':'DET'},\n",
        "               {'POS':'PROPN','DEP':'compound'},\n",
        "               {'POS':'PROPN','DEP':'compound'},\n",
        "               {'POS':'PROPN','OP':'?'},\n",
        "               {'POS':'PROPN','OP':'?'},\n",
        "               {'POS':'PROPN','OP':'?'},\n",
        "               {'LOWER':{'IN':prog_list},'OP':'+'}\n",
        "              ]\n",
        "    \n",
        "    if check == 0:\n",
        "        # return blank list\n",
        "        return schemes\n",
        "\n",
        "    # Matcher class object \n",
        "    matcher = Matcher(nlp.vocab) \n",
        "    matcher.add(\"matching\", None, pattern) \n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        \n",
        "        # match: id, start, end\n",
        "        start, end = matches[i][1], matches[i][2]\n",
        "        \n",
        "        if doc[start].pos_=='DET':\n",
        "            start = start+1\n",
        "        \n",
        "        # matched string\n",
        "        span = str(doc[start:end])\n",
        "        \n",
        "        if (len(schemes)!=0) and (schemes[-1] in span):\n",
        "            schemes[-1] = span\n",
        "        else:\n",
        "            schemes.append(span)\n",
        "        \n",
        "    return schemes\n",
        "\n",
        "# apply function\n",
        "df2['Schemes1'] = df2.apply(lambda x:all_schemes(x.Sent,x.Check_Schemes),axis=1)\n"
      ],
      "metadata": {
        "id": "q4aUtxxKfnrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(' Last year, I spoke about the Ujjwala programme , through which, I am happy to report, 50 million free liquid-gas connections have been provided so far')\n",
        "png = visualise_spacy_tree.create_png(doc)\n",
        "display(Image(png))"
      ],
      "metadata": {
        "id": "zoFTIKAQfnz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# rule to extract initiative name\n",
        "def sent_subtree(text):\n",
        "    \n",
        "    # pattern match for schemes or initiatives\n",
        "    patterns = [r'\\b(?i)'+'plan'+r'\\b',\n",
        "           r'\\b(?i)'+'programme'+r'\\b',\n",
        "           r'\\b(?i)'+'scheme'+r'\\b',\n",
        "           r'\\b(?i)'+'campaign'+r'\\b',\n",
        "           r'\\b(?i)'+'initiative'+r'\\b',\n",
        "           r'\\b(?i)'+'conference'+r'\\b',\n",
        "           r'\\b(?i)'+'agreement'+r'\\b',\n",
        "           r'\\b(?i)'+'alliance'+r'\\b']\n",
        "    \n",
        "    schemes = []\n",
        "    doc = nlp(text)\n",
        "    flag = 0\n",
        "    # if no initiative present in sentence\n",
        "    for pat in patterns:\n",
        "        \n",
        "        if re.search(pat, text) != None:\n",
        "            flag = 1\n",
        "            break\n",
        "    \n",
        "    if flag == 0:\n",
        "        return schemes\n",
        "\n",
        "    # iterating over sentence tokens\n",
        "    for token in doc:\n",
        "\n",
        "        for pat in patterns:\n",
        "                \n",
        "            # if we get a pattern match\n",
        "            if re.search(pat, token.text) != None:\n",
        "\n",
        "                word = ''\n",
        "                # iterating over token subtree\n",
        "                for node in token.subtree:\n",
        "                    # only extract the proper nouns\n",
        "                    if (node.pos_ == 'PROPN'):\n",
        "                        word += node.text+' '\n",
        "\n",
        "                if len(word)!=0:\n",
        "                    schemes.append(word)\n",
        "\n",
        "    return schemes      \n",
        "\n",
        "# derive initiatives\n",
        "df2['Schemes2'] = df2['Sent'].apply(sent_subtree)\n"
      ],
      "metadata": {
        "id": "dVk5AXOAf1r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_list = []\n",
        "# df2 contains all sentences from all speeches\n",
        "for i in range(len(df2)):\n",
        "    sent = df2.loc[i,'Sent']\n",
        "    \n",
        "    if (',' not in sent) and (len(sent.split()) <= 15):\n",
        "        \n",
        "        year = df2.loc[i,'Year']\n",
        "        length = len(sent.split())\n",
        "        \n",
        "        dict1 = {'Year':year,'Sent':sent,'Len':length}\n",
        "        row_list.append(dict1)\n",
        "        \n",
        "# df with shorter sentences\n",
        "df3 = pd.DataFrame(columns=['Year','Sent',\"Len\"])\n",
        "df3 = pd.DataFrame(row_list)"
      ],
      "metadata": {
        "id": "7x_eQbw7KpTQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "def rand_sent(df):\n",
        "    \n",
        "    index = randint(0, len(df))\n",
        "    print('Index = ',index)\n",
        "    doc = nlp(df.loc[index,'Sent'][1:])\n",
        "    displacy.render(doc, style='dep',jupyter=True)\n",
        "    \n",
        "    return index"
      ],
      "metadata": {
        "id": "qIa8bLOOKrNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check output percentage for a rule\n",
        "def output_per(df,out_col):\n",
        "    \n",
        "    result = 0\n",
        "    \n",
        "    for out in df[out_col]:\n",
        "        if len(out)!=0:\n",
        "            result+=1\n",
        "    \n",
        "    per = result/len(df)\n",
        "    per *= 100\n",
        "    \n",
        "    return per"
      ],
      "metadata": {
        "id": "kmj2OB6hKtgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for rule 1: noun(subject), verb, noun(object)\n",
        "def rule1(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        # if the token is a verb\n",
        "        if (token.pos_=='VERB'):\n",
        "            \n",
        "            phrase =''\n",
        "            \n",
        "            # only extract noun or pronoun subjects\n",
        "            for sub_tok in token.lefts:\n",
        "                \n",
        "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
        "                    \n",
        "                    # add subject to the phrase\n",
        "                    phrase += sub_tok.text\n",
        "\n",
        "                    # save the root of the verb in phrase\n",
        "                    phrase += ' '+token.lemma_ \n",
        "\n",
        "                    # check for noun or pronoun direct objects\n",
        "                    for sub_tok in token.rights:\n",
        "                        \n",
        "                        # save the object in the phrase\n",
        "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
        "                                    \n",
        "                            phrase += ' '+sub_tok.text\n",
        "                            sent.append(phrase)\n",
        "            \n",
        "    return sent"
      ],
      "metadata": {
        "id": "W3pZlaY4KxY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 1\n",
        "row_list = []\n",
        "\n",
        "for i in range(len(df3)):\n",
        "    \n",
        "    sent = df3.loc[i,'Sent']\n",
        "    year = df3.loc[i,'Year']\n",
        "    output = rule1(sent)\n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "    \n",
        "df_rule1 = pd.DataFrame(row_list)\n",
        "\n",
        "# rule 1 achieves 20% result on simple sentences\n",
        "output_per(df_rule1,'Output')"
      ],
      "metadata": {
        "id": "uW8lfIaPKyu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 1\n",
        "row_list = []\n",
        "\n",
        "# df2 contains all the sentences from all the speeches\n",
        "for i in range(len(df2)):\n",
        "    \n",
        "    sent = df2.loc[i,'Sent']\n",
        "    year = df2.loc[i,'Year']\n",
        "    output = rule1(sent)\n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "    \n",
        "df_rule1_all = pd.DataFrame(row_list)\n",
        "\n",
        "# check rule1 output on complete speeches\n",
        "output_per(df_rule1_all,'Output')"
      ],
      "metadata": {
        "id": "M-gdTH2IK0Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting non-empty output rows\n",
        "df_show = pd.DataFrame(columns=df_rule1_all.columns)\n",
        "\n",
        "for row in range(len(df_rule1_all)):\n",
        "    \n",
        "    if len(df_rule1_all.loc[row,'Output'])!=0:\n",
        "        df_show = df_show.append(df_rule1_all.loc[row,:])\n",
        "\n",
        "# reset the index\n",
        "df_show.reset_index(inplace=True)\n",
        "df_show.drop('index',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "4Xw2d3JoK1SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate subject, verb and object\n",
        "\n",
        "verb_dict = dict()\n",
        "dis_dict = dict()\n",
        "dis_list = []\n",
        "\n",
        "# iterating over all the sentences\n",
        "for i in range(len(df_show)):\n",
        "    \n",
        "    # sentence containing the output\n",
        "    sentence = df_show.loc[i,'Sent']\n",
        "    # year of the sentence\n",
        "    year = df_show.loc[i,'Year']\n",
        "    # output of the sentence\n",
        "    output = df_show.loc[i,'Output']\n",
        "    \n",
        "    # iterating over all the outputs from the sentence\n",
        "    for sent in output:\n",
        "        \n",
        "        # separate subject, verb and object\n",
        "        n1, v, n2 = sent.split()[:1], sent.split()[1], sent.split()[2:]\n",
        "        \n",
        "        # append to list, along with the sentence\n",
        "        dis_dict = {'Sent':sentence,'Year':year,'Noun1':n1,'Verb':v,'Noun2':n2}\n",
        "        dis_list.append(dis_dict)\n",
        "        \n",
        "        # counting the number of sentences containing the verb\n",
        "        verb = sent.split()[1]\n",
        "        if verb in verb_dict:\n",
        "            verb_dict[verb]+=1\n",
        "        else:\n",
        "            verb_dict[verb]=1\n",
        "\n",
        "df_sep = pd.DataFrame(dis_list)"
      ],
      "metadata": {
        "id": "0aHrQMCqK2tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for rule 2\n",
        "def rule2(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "\n",
        "    pat = []\n",
        "    \n",
        "    # iterate over tokens\n",
        "    for token in doc:\n",
        "        phrase = ''\n",
        "        # if the word is a subject noun or an object noun\n",
        "        if (token.pos_ == 'NOUN')\\\n",
        "            and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass']):\n",
        "            \n",
        "            # iterate over the children nodes\n",
        "            for subtoken in token.children:\n",
        "                # if word is an adjective or has a compound dependency\n",
        "                if (subtoken.pos_ == 'ADJ') or (subtoken.dep_ == 'compound'):\n",
        "                    phrase += subtoken.text + ' '\n",
        "                    \n",
        "            if len(phrase)!=0:\n",
        "                phrase += token.text\n",
        "             \n",
        "        if  len(phrase)!=0:\n",
        "            pat.append(phrase)\n",
        "        \n",
        "    \n",
        "    return pat"
      ],
      "metadata": {
        "id": "HleCCgakMMET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 2\n",
        "row_list = []\n",
        "\n",
        "for i in range(len(df3)):\n",
        "    \n",
        "    sent = df3.loc[i,'Sent']\n",
        "    year = df3.loc[i,'Year']\n",
        "    # rule\n",
        "    output = rule2(sent)\n",
        "    \n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "\n",
        "df_rule2 = pd.DataFrame(row_list)"
      ],
      "metadata": {
        "id": "HqIkEMIHMNWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 2\n",
        "row_list = []\n",
        "\n",
        "# df2 contains all the sentences from all the speeches\n",
        "for i in range(len(df2)):\n",
        "    \n",
        "    sent = df2.loc[i,'Sent']\n",
        "    year = df2.loc[i,'Year']\n",
        "    output = rule2(sent)\n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "    \n",
        "df_rule2_all = pd.DataFrame(row_list)\n",
        "\n",
        "# check rule output on complete speeches\n",
        "output_per(df_rule2_all,'Output')"
      ],
      "metadata": {
        "id": "XQmyBcdAMOsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting non-empty outputs\n",
        "df_show2 = pd.DataFrame(columns=df_rule2_all.columns)\n",
        "\n",
        "for row in range(len(df_rule2_all)):\n",
        "    \n",
        "    if len(df_rule2_all.loc[row,'Output'])!=0:\n",
        "        df_show2 = df_show2.append(df_rule2_all.loc[row,:])\n",
        "\n",
        "# reset the index\n",
        "df_show2.reset_index(inplace=True)\n",
        "df_show2.drop('index',axis=1,inplace=True)  "
      ],
      "metadata": {
        "id": "-ZT6jrucMU_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rule2_mod(text,index):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "\n",
        "    phrase = ''\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        if token.i == index:\n",
        "            \n",
        "            for subtoken in token.children:\n",
        "                if (subtoken.pos_ == 'ADJ'):\n",
        "                    phrase += ' '+subtoken.text\n",
        "            break\n",
        "    \n",
        "    return phrase"
      ],
      "metadata": {
        "id": "ojnTS59_MWVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rule 1 modified function\n",
        "def rule1_mod(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "        # root word\n",
        "        if (token.pos_=='VERB'):\n",
        "            \n",
        "            phrase =''\n",
        "            \n",
        "            # only extract noun or pronoun subjects\n",
        "            for sub_tok in token.lefts:\n",
        "                \n",
        "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
        "                    \n",
        "                    # look for subject modifier\n",
        "                    adj = rule2_mod(text,sub_tok.i)\n",
        "                    \n",
        "                    phrase += adj + ' ' + sub_tok.text\n",
        "\n",
        "                    # save the root word of the word\n",
        "                    phrase += ' '+token.lemma_ \n",
        "\n",
        "                    # check for noun or pronoun direct objects\n",
        "                    for sub_tok in token.rights:\n",
        "                        \n",
        "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
        "                            \n",
        "                            # look for object modifier\n",
        "                            adj = rule2_mod(text,sub_tok.i)\n",
        "                            \n",
        "                            phrase += adj+' '+sub_tok.text\n",
        "                            sent.append(phrase)\n",
        "            \n",
        "    return sent"
      ],
      "metadata": {
        "id": "7bILML5-MYUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for modified rule 1\n",
        "row_list = []\n",
        "\n",
        "# df2 contains all the sentences from all the speeches\n",
        "for i in range(len(df2)):\n",
        "    \n",
        "    sent = df2.loc[i,'Sent']\n",
        "    year = df2.loc[i,'Year']\n",
        "    output = rule1_mod(sent)\n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "    \n",
        "df_rule1_mod_all = pd.DataFrame(row_list)\n",
        "# check rule1 output on complete speeches\n",
        "output_per(df_rule1_mod_all,'Output')"
      ],
      "metadata": {
        "id": "2FqJkQmYMZk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rule 3 function\n",
        "def rule3(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "\n",
        "        # look for prepositions\n",
        "        if token.pos_=='ADP':\n",
        "\n",
        "            phrase = ''\n",
        "            \n",
        "            # if its head word is a noun\n",
        "            if token.head.pos_=='NOUN':\n",
        "                \n",
        "                # append noun and preposition to phrase\n",
        "                phrase += token.head.text\n",
        "                phrase += ' '+token.text\n",
        "\n",
        "                # check the nodes to the right of the preposition\n",
        "                for right_tok in token.rights:\n",
        "                    # append if it is a noun or proper noun\n",
        "                    if (right_tok.pos_ in ['NOUN','PROPN']):\n",
        "                        phrase += ' '+right_tok.text\n",
        "                \n",
        "                if len(phrase)>2:\n",
        "                    sent.append(phrase)\n",
        "                \n",
        "    return sent"
      ],
      "metadata": {
        "id": "KMmuBxUOMebo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 3\n",
        "row_list = []\n",
        "\n",
        "for i in range(len(df3)):\n",
        "    \n",
        "    sent = df3.loc[i,'Sent']\n",
        "    year = df3.loc[i,'Year']\n",
        "    \n",
        "    # rule\n",
        "    output = rule3(sent)\n",
        "    \n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "\n",
        "df_rule3 = pd.DataFrame(row_list)\n",
        "# output percentage for rule 3\n",
        "output_per(df_rule3,'Output')"
      ],
      "metadata": {
        "id": "WWbh97HcMf40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df containing sentence and its output for rule 3\n",
        "row_list = []\n",
        "\n",
        "# df2 contains all the sentences from all the speeches\n",
        "for i in range(len(df2)):\n",
        "    \n",
        "    sent = df2.loc[i,'Sent']\n",
        "    year = df2.loc[i,'Year']\n",
        "    output = rule3(sent)\n",
        "    dict1 = {'Year':year,'Sent':sent,'Output':output}\n",
        "    row_list.append(dict1)\n",
        "    \n",
        "df_rule3_all = pd.DataFrame(row_list)\n",
        "# check rule3 output on complete speeches\n",
        "output_per(df_rule3_all,'Output')"
      ],
      "metadata": {
        "id": "BI0iQn1tMhTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select non-empty outputs\n",
        "df_show3 = pd.DataFrame(columns=df_rule3_all.columns)\n",
        "\n",
        "for row in range(len(df_rule3_all)):\n",
        "    \n",
        "    if len(df_rule3_all.loc[row,'Output'])!=0:\n",
        "        df_show3 = df_show3.append(df_rule3_all.loc[row,:])\n",
        "\n",
        "# reset the index\n",
        "df_show3.reset_index(inplace=True)\n",
        "df_show3.drop('index',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "7av4T5llMiT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate noun, preposition and noun\n",
        "\n",
        "prep_dict = dict()\n",
        "dis_dict = dict()\n",
        "dis_list = []\n",
        "\n",
        "# iterating over all the sentences\n",
        "for i in range(len(df_show3)):\n",
        "    \n",
        "    # sentence containing the output\n",
        "    sentence = df_show3.loc[i,'Sent']\n",
        "    # year of the sentence\n",
        "    year = df_show3.loc[i,'Year']\n",
        "    # output of the sentence\n",
        "    output = df_show3.loc[i,'Output']\n",
        "    \n",
        "    # iterating over all the outputs from the sentence\n",
        "    for sent in output:\n",
        "        \n",
        "        # separate subject, verb and object\n",
        "        n1, p, n2 = sent.split()[0], sent.split()[1], sent.split()[2:]\n",
        "        \n",
        "        # append to list, along with the sentence\n",
        "        dis_dict = {'Sent':sentence,'Year':year,'Noun1':n1,'Preposition':p,'Noun2':n2}\n",
        "        dis_list.append(dis_dict)\n",
        "        \n",
        "        # counting the number of sentences containing the verb\n",
        "        prep = sent.split()[1]\n",
        "        if prep in prep_dict:\n",
        "            prep_dict[prep]+=1\n",
        "        else:\n",
        "            prep_dict[prep]=1\n",
        "\n",
        "df_sep3= pd.DataFrame(dis_list)"
      ],
      "metadata": {
        "id": "g4GAkwKgMjvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sort = sorted(prep_dict.items(), key = lambda d:(d[1],d[0]), reverse=True)\n",
        "sort[:10]"
      ],
      "metadata": {
        "id": "dKp32bPVMk0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rule 0\n",
        "def rule0(text, index):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "        \n",
        "    token = doc[index]\n",
        "    \n",
        "    entity = ''\n",
        "    \n",
        "    for sub_tok in token.children:\n",
        "        if (sub_tok.dep_ in ['compound','amod']):\n",
        "            entity += sub_tok.text+' '\n",
        "    \n",
        "    entity += token.text\n",
        "\n",
        "    return entity"
      ],
      "metadata": {
        "id": "GSwGwQ5qMn8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rule 3 function\n",
        "def rule3_mod(text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "\n",
        "        if token.pos_=='ADP':\n",
        "\n",
        "            phrase = ''\n",
        "            if token.head.pos_=='NOUN':\n",
        "                \n",
        "                # appended rule\n",
        "                append = rule0(text, token.head.i)\n",
        "                if len(append)!=0:\n",
        "                    phrase += append\n",
        "                else:  \n",
        "                    phrase += token.head.text\n",
        "                phrase += ' '+token.text\n",
        "\n",
        "                for right_tok in token.rights:\n",
        "                    if (right_tok.pos_ in ['NOUN','PROPN']):\n",
        "                        \n",
        "                        right_phrase = ''\n",
        "                        # appended rule\n",
        "                        append = rule0(text, right_tok.i)\n",
        "                        if len(append)!=0:\n",
        "                            right_phrase += ' '+append\n",
        "                        else:\n",
        "                            right_phrase += ' '+right_tok.text\n",
        "                            \n",
        "                        phrase += right_phrase\n",
        "                \n",
        "                if len(phrase)>2:\n",
        "                    sent.append(phrase)\n",
        "                \n",
        "\n",
        "    return sent"
      ],
      "metadata": {
        "id": "Mh5SUIm3Mpps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}